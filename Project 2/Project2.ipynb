{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf6e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "#Downloads stopwords which can be avoided during data pre-processing stage\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6cf887",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stop_words = [\"why's\", '5th', 'subject:', 'from:', 'newsgroups:', 'lines:', 'believe', 'references:', \n",
    "                         'writes:', '3rd', \"she'd\", '8th', 'hundred', 'world', \"he'll\", 'university', '9th', '1st', \n",
    "                         'ours6th', 'ten', \"when's\", 'stillwould', 'message-id:', '4th', 'thousand', \n",
    "                         'nntp-posting-host:', 'article', 'organization:', 'cantaloupe.srv.cs.cmu.edu', '7th', \n",
    "                         \"10thi've\", 'date:', \"he'd\", '2nd', 'sender:', 'distribution:', \"how's\", 'path:', \"she'll\", \n",
    "                         'xref:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e778cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    folders = os.listdir(folder_path)\n",
    "    data_dict = {}\n",
    "    news_groups_dict = {}\n",
    "    for folder in folders:\n",
    "        if not folder.startswith('.'):\n",
    "            data_dict[folder] = []\n",
    "            sub_folder_path = os.path.join(folder_path, folder)\n",
    "            news_groups_dict[folder] = sub_folder_path\n",
    "            for file in os.listdir(sub_folder_path):\n",
    "                with open(os.path.join(sub_folder_path, file), encoding='latin-1') as file_stream:\n",
    "                    data_dict[folder].append(file_stream.read())\n",
    "                    file_stream.close()\n",
    "    return data_dict, news_groups_dict\n",
    "    \n",
    "    \n",
    "def load_corpus():\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    stopwords_list.update(list(punctuation))\n",
    "    stopwords_list.update(additional_stop_words)\n",
    "    return stopwords_list\n",
    "    \n",
    "    \n",
    "def get_word_frequency(newsdata, stopwords_list, word_frequency_dict):\n",
    "    file_frequency_dict = {}\n",
    "    for idx, doc in enumerate(newsdata):\n",
    "        if idx not in file_frequency_dict:\n",
    "            file_frequency_dict[idx] = {}\n",
    "        for words in doc.split():\n",
    "            words = process_words(words, stopwords_list)\n",
    "            if words != []:\n",
    "                word_frequency_dict[words] = word_frequency_dict.get(words, 1) + 1\n",
    "                file_frequency_dict[idx][words] = file_frequency_dict.get(idx).get(words, 1) + 1\n",
    "    return word_frequency_dict, file_frequency_dict\n",
    "    \n",
    "    \n",
    "def process_words(data, stopwords_list):\n",
    "    lowercase_data = data.lower()\n",
    "    if lowercase_data not in stopwords_list:\n",
    "        data = data\n",
    "        return lowercase_data\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "def get_dataframe(news_groups_dict, data_dict, features, file_dict):\n",
    "    newsgroups_dataframe = pd.DataFrame(columns = features)\n",
    "    for folder, data in tqdm(data_dict.items()):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "        sub_folder_path = news_groups_dict[folder]\n",
    "        for idx, file in enumerate(data_dict[folder]):\n",
    "            newsgroups_dataframe.loc[len(newsgroups_dataframe)] = [file_dict[folder][idx].get(x, 1) \n",
    "                                                                   for x in features]\n",
    "    return newsgroups_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686e68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/samee/Downloads/bow-daraset/20_newsgroups'\n",
    "data_dict, news_groups_dict = load_data(data_path)\n",
    "stopwords_list = load_corpus()\n",
    "wf_dict = {}\n",
    "file_dict = {}\n",
    "\n",
    "for group, data in data_dict.items():\n",
    "    wf_dict, file_dict_temp = get_word_frequency(data, stopwords_list, wf_dict)\n",
    "    file_dict[group] = file_dict_temp\n",
    "\n",
    "wf_list = (sorted(wf_dict.items(), key=operator.itemgetter(1), reverse=True))\n",
    "features_list = [word[0] for word in wf_list][:1000]\n",
    "class_list = list(file_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f2ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [13:13<00:00, 39.69s/it]\n"
     ]
    }
   ],
   "source": [
    "wf_data_df=get_dataframe(news_groups_dict, data_dict, features_list, file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5cafd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesModel:\n",
    "    \n",
    "    def __init__(self, data_df, class_list, features, file_dict):\n",
    "        self._model = {}\n",
    "        self.data_df = data_df\n",
    "        self.class_list = class_list\n",
    "        self.features = features\n",
    "        self.file_dict = file_dict\n",
    "        self.n_classes = len(self.class_list)\n",
    "        self.n_features = len(self.features)\n",
    "        self.class_list = class_list\n",
    "        self.flag_fit = False\n",
    "        \n",
    "    def fit(self, test_size=0.3):\n",
    "        self.X = self.data_df.values\n",
    "        list_labels = []\n",
    "        for group, data in self.file_dict.items():\n",
    "            list_labels.extend([group]*len(data))\n",
    "        self.y = np.array(list_labels)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, \n",
    "                                                                                self.y, \n",
    "                                                                                test_size=\n",
    "                                                                                test_size)\n",
    "        self._model['Number_of_classes'] = len(self.y_train)\n",
    "        for label in self.class_list:\n",
    "            total_words = 1\n",
    "            self._model[label] = {}\n",
    "            current_rows = self.y_train == label\n",
    "            X_train_current = self.X_train[current_rows]\n",
    "            for feauture in range(len(self.features)):\n",
    "                sumoffeatures = X_train_current[:, feauture].sum()\n",
    "                self._model[label][self.features[feauture]] = sumoffeatures\n",
    "                total_words += sumoffeatures\n",
    "            self._model[label][\"Number_of_classes\"] = total_words\n",
    "        print('Naive Bayes Model fitting done.')\n",
    "        self.flag_fit = True\n",
    "        return None\n",
    "    \n",
    "    def _calc_probability_(self, word, classgroup):\n",
    "        if not self.flag_fit:\n",
    "            self.fit()\n",
    "        class_count = self._model[classgroup][\"Number_of_classes\"]\n",
    "        log_class_count = np.log(class_count)\n",
    "        total_count = self._model[\"Number_of_classes\"]\n",
    "        log_total_count = np.log(total_count)\n",
    "        current_out = log_class_count - log_total_count\n",
    "        for idx in range(len(self.features)):\n",
    "            current_word_count = self._model[classgroup][self.features[idx]] + 1\n",
    "            log_current_word_count = np.log(current_word_count)\n",
    "            total_word_count=  self._model[classgroup][\"Number_of_classes\"] + len(self.features)\n",
    "            log_total_word_count = np.log(total_word_count)\n",
    "            curr_probability = log_current_word_count-log_total_word_count\n",
    "            for _ in range(int(word[idx])):\n",
    "                current_out = curr_probability + current_out\n",
    "        return current_out\n",
    "    \n",
    "    def __predict__(self, word):\n",
    "        prev_probability = -30000000\n",
    "        prev_class = -300000\n",
    "        for current in self.file_dict.keys():\n",
    "            if current == 'Number_of_classes':\n",
    "                continue\n",
    "            curr_probability = self._calc_probability_(word, current)\n",
    "            if(curr_probability > prev_probability):\n",
    "                prev_class = current\n",
    "                prev_probability = curr_probability\n",
    "            is_first_exec = False\n",
    "        return prev_class\n",
    "    \n",
    "    def predict_batch(self, word_list=None):\n",
    "        if not self.flag_fit:\n",
    "            self.fit()\n",
    "        if word_list is None:\n",
    "            word_list = self.X_test\n",
    "        class_predicted = []\n",
    "        for curr_word in tqdm(word_list):\n",
    "            class_predicted.append(self.__predict__(curr_word))\n",
    "        return class_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1106ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesModel(wf_data_df, class_list, features_list, file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903a8a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model fitting done.\n"
     ]
    }
   ],
   "source": [
    "model.fit(test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a07684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7999/7999 [17:37<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "class_predicted=model.predict_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9070087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[150   7   3   1   5   8   1   3   6   8   5   1   5  10   4   3   0  12\n",
      "    6  49]\n",
      " [  0 167  34   7   0  54   0   1   0   0   0   3   6   4   8   0   0   0\n",
      "    1   0]\n",
      " [  0   0  40   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   5  33 239  32   5  18   2   0   0   1   2   3   0   1   0   0   1\n",
      "    1   0]\n",
      " [  2  15  55  15 327  20   7   2   0   0   0   5   2   0   2   0   0   0\n",
      "    3   3]\n",
      " [  0   4   2   3   0  78   0   0   0   0   0   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [ 24  77  93  61  17  82 336  24  24  45  72  21  20  38  24  39  35  36\n",
      "   45  34]\n",
      " [  8   5   4   6   1   1   8 294   5   2   5   2   5   5   8   0  10   8\n",
      "    9   7]\n",
      " [ 57  12  14   5   2   9   3  12 330   8  58  32   3   9  15   5  39  52\n",
      "   48  35]\n",
      " [  1   0   1   2   0   0   2   0   0 297 154   0   0   0   2   0   0   0\n",
      "    2   1]\n",
      " [  0   4   1   0   1   1   6   0   0  13  43   1   4   1   2   0   0   0\n",
      "    0   0]\n",
      " [  3   1   7   1   0   8   1   1   1   1   0 227   2   0   1   1   2   2\n",
      "    2   0]\n",
      " [ 34  54  81  43  10 100  12  15   4  13  31  42 312  19  27  20  38  29\n",
      "   34  34]\n",
      " [ 24  14  17   4   2  23   5   3   4   4  11  20   4 271  20   5  19  48\n",
      "   61  35]\n",
      " [  5   3   7   1   1   6   0   3   1   0   1   4   1   2 248   0   2   4\n",
      "    8   5]\n",
      " [ 24   1   0   0   0   0   0   0   0   0   0   0   0   3   2 335   0   1\n",
      "    1  27]\n",
      " [  6   3   7   4   2   0   1   9   3   2   5  13   9   4   9   1 238  10\n",
      "   62  30]\n",
      " [ 16   4   4   1   1  16   1   2   4   6   5   1   7   5   4   2   5 171\n",
      "   12  14]\n",
      " [ 19   1   2   2   1   1   0   6   4   3   9  15   2  15  11   0  19  13\n",
      "  105  26]\n",
      " [ 53   1   0   2   0   1   1   0   2   1   4   0   1   5   3   4   8   8\n",
      "   15 103]]\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.35      0.52      0.42       287\n",
      "           comp.graphics       0.44      0.59      0.50       285\n",
      " comp.os.ms-windows.misc       0.10      0.95      0.18        42\n",
      "comp.sys.ibm.pc.hardware       0.60      0.70      0.65       343\n",
      "   comp.sys.mac.hardware       0.81      0.71      0.76       458\n",
      "          comp.windows.x       0.19      0.88      0.31        89\n",
      "            misc.forsale       0.83      0.29      0.43      1147\n",
      "               rec.autos       0.78      0.75      0.76       393\n",
      "         rec.motorcycles       0.85      0.44      0.58       748\n",
      "      rec.sport.baseball       0.74      0.64      0.69       462\n",
      "        rec.sport.hockey       0.11      0.56      0.18        77\n",
      "               sci.crypt       0.58      0.87      0.70       261\n",
      "         sci.electronics       0.81      0.33      0.47       952\n",
      "                 sci.med       0.69      0.46      0.55       594\n",
      "               sci.space       0.63      0.82      0.72       302\n",
      "  soc.religion.christian       0.81      0.85      0.83       394\n",
      "      talk.politics.guns       0.57      0.57      0.57       418\n",
      "   talk.politics.mideast       0.43      0.61      0.51       281\n",
      "      talk.politics.misc       0.25      0.41      0.31       254\n",
      "      talk.religion.misc       0.26      0.49      0.33       212\n",
      "\n",
      "                accuracy                           0.54      7999\n",
      "               macro avg       0.54      0.62      0.52      7999\n",
      "            weighted avg       0.67      0.54      0.56      7999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(class_predicted,model.y_test))\n",
    "print(classification_report(class_predicted,model.y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
